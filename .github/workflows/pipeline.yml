# This exists as our open data pipeline.
# It runs the opendata-pipeline CLI app with some enhancements for CI/CD.

# ! Actually can't we just pipx install the program??
# assuming the latest version has been published... and we can still `checkout`
# ^ do we even need to checkout?? can't we just `--use-remote`???

name: OpenData Pipeline

# Controls when the workflow will run
on:
  # want to trigger on CRON or manually
  # CRON....
  # TODO: add CRON

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# making executive decision to setup the project in multiple jobs so that we can run in parallel
# as opposed to one `poetry install` but sequential running

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # fetch the data
  fetch-data:
    name: Fetch Data

    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout
        uses: actions/checkout@v3

      # setup python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
          cache: "poetry"

      # install dependencies and package
      - name: Install Dependencies
        run: poetry install

      # initialize data dir
      - name: Initialize App
        run: poetry run opendata-pipeline init

      # fetch records
      - name: Run fetching
        # here we can read local (which is latest remote)
        # BUT we still need to update the remote
        run: poetry run opendata-pipeline fetch --update-remote

      - name: Upload Drug Prep Artifact
        uses: actions/upload-artifact@v3
        with:
          name: drug-files
          path: data/*_drug_prep.csv

      - name: Upload Records Artifact
        uses: actions/upload-artifact@v3
        with:
          name: records-files
          path: data/*_records.jsonl

      - name: Finish
        run: echo "Done."

  # extract drugs from the data
  extract-drugs:
    name: Extract Drugs

    # requires data to be fetched
    needs: fetch-data

    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout
        uses: actions/checkout@v3

      # setup python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
          cache: "poetry"

      # install dependencies and package
      - name: Install Dependencies
        run: poetry install

      # download drug_prep records (should be in data folder)
      - name: Download Drug Prep Artifact
        uses: actions/download-artifact@v3
        with:
          name: drug-files

      # install rust CLI program
      - name: Install Drug Extraction Toolbox
        # this is by rust community... will get caching soon? link to PR -> https://github.com/actions-rs/meta/issues/21
        uses: actions-rs/install@v0.1
        with:
          crate: drug-extraction-cli
          version: latest
          use-tool-cache: true
        run: extract-drugs --version

      - name: Run drug extraction
        # again we don't need to `--use-remote` because we checked it out
        run: poetry run opendata-pipeline extract-drugs

      # Upload drug data file artifact
      - uses: actions/upload-artifact@v3
        with:
          name: drug-output
          path: data/drug_data.jsonl

      - name: Finish
        run: echo "Done."

  # geocode the data
  geocode:
    name: Geocode

    # requires data to be fetched
    needs: fetch-data

    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout
        uses: actions/checkout@v3

      # setup python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
          cache: "poetry"

      # install dependencies and package
      - name: Install Dependencies
        run: poetry install

      # download records (should be in data folder)
      - name: Download Records Artifact
        uses: actions/download-artifact@v3
        with:
          name: records-files

      - name: Run geocoding
        # again we don't need to `--use-remote` because we checked it out
        run: poetry run opendata-pipeline geocode
        # use env here to scope to this command
        env:
          # get from github secrets
          ARCGIS_API_KEY: ${{ secrets.ARCGIS_TOKEN }}

      # Upload geocoding file artifact
      - uses: actions/upload-artifact@v3
        with:
          name: geocoding-output
          path: data/geocoded_data.jsonl

      - name: Finish
        run: echo "Done."

  # analyze/combine the data
  analyze:
    name: Analyze

    # requires data to be fetched
    needs: [extract-drugs, geocode]

    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout
        uses: actions/checkout@v3

      # setup python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
          cache: "poetry"

      # install dependencies and package
      - name: Install Dependencies
        run: poetry install

      # download records (should be in data folder)
      - name: Download Records Artifact
        uses: actions/download-artifact@v3
        with:
          name: records-files

      - name: Run geocoding
        # again we don't need to `--use-remote` because we checked it out
        run: poetry run opendata-pipeline geocode
        # use env here to scope to this command
        env:
          # get from github secrets
          ARCGIS_API_KEY: ${{ secrets.ARCGIS_TOKEN }}

      # Upload geocoding file artifact
      - uses: actions/upload-artifact@v3
        with:
          name: geocoding-output
          path: data/geocoded_data.jsonl

      - name: Finish
        run: echo "Done."
